<!DOCTYPE HTML>
<html lang="en">
  
<head>
  <meta name=viewport content=‚Äúwidth=800‚Äù>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <!-- <style type="text/css">
    @import url('https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap');
      /* Color scheme stolen from Sergey Karayev */
      a {
      color: #B83A4B;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Ubuntu', sans-serif;
      font-size: 15px;
      font-weight: 400;
      font-style: normal;
      }
      strong {
      font-family: 'Ubuntu', sans-serif;
      font-size: 15px;
      font-weight: 500;
      font-style: normal;
      }
      heading {
      font-family: 'Ubuntu', sans-serif;
      font-size: 24px;
      font-weight: 500;
      font-style: normal;
      }
      papertitle {
      font-family: 'Ubuntu', sans-serif;
      font-size: 15px;
      font-weight:500;
      font-style: normal;
      }
      name {
      font-family: 'Ubuntu', sans-serif;
      font-weight: 400;
      font-size: 32px;
      font-style: normal;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
  </style> -->
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #B83A4B;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th,tr,p,a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
      }
      strong {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 400;
      }
      heading {
      font-family: 'Roboto', sans-serif;
      font-size: 24px;
      font-weight: 400;
      }
      papertitle {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight:500;
      }
      name {
      font-family: 'Roboto', sans-serif;
      font-weight: 400;
      font-size: 32px;
      }
      .one
      {
      width: 160px;
      height: 140px;
      position: relative;
      }
      .two
      {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
      }
      .fade {
       transition: opacity .2s ease-in-out;
       -moz-transition: opacity .2s ease-in-out;
       -webkit-transition: opacity .2s ease-in-out;
      }
      span.highlight {
          background-color: #ffffd0;
      }
  </style>
  <link rel="icon" type="image/png" href="media/preview.jpg">
  <title>Sayan Deb Sarkar - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <script src="script/functions.js"></script>
</head>

<body>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sayan Deb Sarkar</name>
              </p>
              <p>I'm a 2nd-year PhD student at Stanford University in the <a href="https://gradientspaces.stanford.edu/">Gradient Spaces Group</a>, 
                advised by Prof. <a href="https://ir0.github.io/">Iro Armeni</a>, 
                part of the <a href="https://svl.stanford.edu/">Stanford Vision Lab (SVL)</a>. In summer '25, I interned with the <a href="https://www.microsoft.com/en-us/research/lab/spatial-ai-zurich/">Microsoft Spatial AI Lab</a>, working on efficient video understanding in spatial context. 
              <p>
                Before starting PhD, I was a CS master student at <a href="https://ethz.ch/en.html">ETH Z√ºrich</a>, supervised by Prof. <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>, working on 
                aligning real-world 3D environments from multi-modal data. I graduated with a Bachelors in 
                Information Technology from Manipal University, India, where I spent time working on face recognition and medical imaging problems.
              </p>

              <p>
                In 2020-21, I spent a wonderful time working with <a href="https://shreyashampali.github.io/">Shreyas Hampali</a> and <a href="https://radmahdi.github.io/Home.html">Mahdi Rad</a> at 
                Prof. <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>'s 
                lab on hand-object pose estimation and monte carlo scene search for 3D scene understanding. 
                I view them as mentors entering research, and strive to learn from them.
              </p>

              <p>
                My research interests are on multimodal 3D scene understanding and interactive editing. I am always looking for research collaborations, get in touch if you have something relevant. 
                If you're around the Bay Area, feel free to reach out for a cup of coffee!
              </p>
              <p style="text-align:center">
                <a href="mailto:sdsarkar@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="data/Resume_SayanDebSarkar.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=T9zPzwoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/sayands/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/debsarkar_sayan">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sayandebsarkar/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/sayandebsarkar.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="14%" valign="middle">
              <a href="https://www.stanford.edu/"><img src="images/media/stanford_logo.png" width="120"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://www.microsoft.com/en-us/research/lab/spatial-ai-zurich/"><img src="images/media/microsoft_logo.png" width="140"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="images/media/eth_logo.png" width="120"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://www.qualcomm.com/"><img src="images/media/qualcomm_logo.png" width="90"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://www.mercedes-benz.com"><img src="images/media/merc_logo.jpeg" width="90"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://www.tugraz.at/institute/icg/home/"><img src="images/media/tugraz_logo.png" width="90"></a>
            </td>
            <td width="14%" valign="middle">
              <a href="https://www.manipal.edu"><img src="images/media/manipal_logo.jpeg" width="60"></a>
            </td>
            

          </tr>
        </table>

        <!-- NEWS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li style="margin: 5px;"><strong>2025-09</strong> <a href="https://sayands.github.io/guideflow3d/">GuideFlow3D</a> accepted to NeurIPS 2025. See you in San Diego!</li>
                  <li style="margin: 5px;"><strong>2025-07</strong> Invited talk(s) on <a href="data/invited_talk_microsoft_2025.pdf">"Scalable Cross-Modal 3D Scene Understanding"</a> at <a href="https://www.microsoft.com/en-us/research/group/perception-and-interaction/">Perception Cloud</a> and <a href="https://imagine-lab.enpc.fr/">Imagine Labs</a>.</li>
                  <li style="margin: 5px;"><strong>2025-06</strong> This summer I will be a research intern at <a href="https://www.microsoft.com/en-us/research/lab/spatial-ai-zurich/">Microsoft Spatial AI Lab</a>. </li>
                  <li style="margin: 5px;"><strong>2025-03</strong> <a href="https://sayands.github.io/crossover/">CrossOver</a> accepted to CVPR 2025 as <strong  style="color: hsl(0, 98%, 49%);">‚ú®Highlight‚ú®</strong> <strong>(top 3%)!</strong> </li>
                  <li style="margin: 5px;"><strong>2024-09</strong> <b><span style="color:#c20000;">Career Update</span></b>: I joined <a href="https://www.stanford.edu/">Stanford University</a> for PhD in Computer Vision.</li>
                  <li style="margin: 5px;"><strong>2023-08</strong> I will be a research intern at <a href="https://www.qualcomm.com/research/extended-reality">Qualcomm XR Labs</a>, Amsterdam for the Fall.</li>
                  <li style="margin: 5px;"><strong>2023-07</strong> <a href="https://sayands.github.io/sgaligner/">SGAligner</a> accepted to ICCV 2023. First <strong>first-author</strong> submission, first accept! </li>
                  <li style="margin: 5px;"><strong>2022-09</strong> I started Computer Science MSc at ETH Zurich. </li>
                  <a href="javascript:void(0);" onclick="toggleBlock('old_news')">---- show more ----</a>
                  <div id="old_news" style="display: none;">
                    <li style="margin: 5px;"><strong>2022-07</strong> <a href="https://arxiv.org/abs/2104.14639">Keypoint Transformer</a> accepted to CVPR 2022 as <strong>Oral!</strong></li>
                    <li style="margin: 5px;"><strong>2021-05</strong> I started at <strong>Mercedes-Benz R & D</strong> as a Computer Vision Research Engineer. </li>
                    <li style="margin: 5px;"><strong>2021-03</strong> <a href="https://arxiv.org/abs/2103.07969">MCSS</a> accepted at CVPR 2021. </li>
                    <li style="margin: 5px;"><strong>2020-06</strong> <a href="https://arxiv.org/abs/2001.02149">General 3D Room Layout from a Single View by Render-and-Compare
                      </a> accepted at ECCV 2020. </li>
                    <li style="margin: 5px;"><strong>2020-01</strong> I started as a Research Assistant at ICG, TU Graz with 
                      Prof. <a href="https://vincentlepetit.github.io/">Vincent Lepetit</a>, supported by a Qualcomm fellowship.</li>
                  </div>
                </div></div>
                </ul>
              </td>
            </tr>
        </tbody></table>

        <!-- PUBLICATIONS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                  My research interests lie at the intersection of Computer Vision and Machine Learning, specifically in the areas of multimodal data representations for spatial understanding. 
                </p>
                
              </p>
              </td>
            </tr>
          </tbody>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/guideflow3d_teaser.gif' width="250">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer</papertitle>
                <br>
                <strong>Sayan Deb Sarkar</strong>, Sinisa Stekovic, Vincent Lepetit, Iro Armeni
                <!-- <br> -->
                <br>
                <a href="" target="_blank"> <i class="fas fa-file-pdf"></i> arXiv</a> |    
                <a href="https://sayands.github.io/guideflow3d/" target="_blank"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="" target="_blank"> <i class="fas fa-video"> </i> Video</a> |
                <a href="https://github.com/GradientSpaces/GuideFlow3D" target="_blank"> <i class="fab fa-github"></i> Code</a>
                <br>
                <em> Neural Information Processing Systems (NeurIPS), 2025 </em> <br>
          
                <p></p>
                <p>
                  3D appearance transfer pipeline robust to strong geometric variations between objects. 
                  </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/sgalignerpp_teaser.gif' width="250">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment</papertitle>
                <br>
                Binod Singh*, <strong>Sayan Deb Sarkar</strong>*, Iro Armeni
                <!-- <br> -->
                <br>
                <a href="https://www.arxiv.org/abs/2509.20401"> <i class="fas fa-file-pdf"></i> arXiv</a> |    
                <a href="https://singhbino3d.github.io/sgpp/"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://www.youtube.com/watch?v=IIw3UrR-JXs&feature=youtu.be"> <i class="fas fa-video"> </i> Video</a> |
                <a href="" target="_blank"> <i class="fab fa-github"></i> Code</a>
                <br>
                <em> arXiv 2025 </em> <br>
          
                <p></p>
                <p>
                  3D Scene Graph alignment framework across modalities using open-vocabulary cues and learned joint embeddings, achieving robust performance under noise and low overlap. 
                  <br><span style="color:#0758ed;">Master Student Project.</span>
                  </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/crossover_teaser.gif' width="250">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>CrossOver: 3D Scene Cross-Modal Alignment</papertitle>
                <br>
                <strong>Sayan Deb Sarkar</strong>, Ondrej Miksik, Marc Pollefeys, D√°niel B√©la Bar√°th, Iro Armeni
                <!-- <br> -->
                <br>
                <a href="https://arxiv.org/pdf/2502.15011"> <i class="fas fa-file-pdf"></i> arXiv</a> |    
                <a href="https://sayands.github.io/crossover/"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://youtu.be/tDrQ4R-F8hk"> <i class="fas fa-video"> </i> Video</a> |
                <a href="https://github.com/GradientSpaces/CrossOver/"> <i class="fab fa-github"></i> Code</a>
                <br>
                <em> Computer Vision and Pattern Recognition (CVPR), 2025 </em> <br>
                üèÜ <papertitle>Highlight</papertitle> (top 3%)
                <br>
              
                <p></p>
                <p>
                  Cross-modal alignment method for 3D scenes that learns a unified, modality-agnostic embedding space, enabling scene-level alignment without semantic annotations.
                  </p>
              </td>
            </tr> 
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/sgaligner_teaser.png' width="250">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>SGAligner: 3D Scene Alignment with Scene Graphs</papertitle>
                <br>
                <strong>Sayan Deb Sarkar</strong>, Ondrej Miksik, Marc Pollefeys, D√°niel B√©la Bar√°th, Iro Armeni
                <!-- <br> -->
                <br>
                <a href="https://arxiv.org/pdf/2304.14880/pdf"> <i class="fas fa-file-pdf"></i> arXiv</a> |       
                <a href="https://sayands.github.io/sgaligner/"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://youtu.be/nBTyZyY9X7I"> <i class="fas fa-video"> </i> Video</a> |
                <a href="https://github.com/sayands/sgaligner"> <i class="fab fa-github"></i> Code</a>
                <br>
                <em> International Conference on Computer Vision (ICCV), 2023 </em>
                <br>
              
                <p></p>
                <p>
                  3D Scene Graph Alignment robust to in-the-wild scenarios powering point cloud registration and map integration.
                  </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/handsformer_arxiv.png' width="250">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation</papertitle>
                <br>
                Shreyas Hampali, <strong>Sayan Deb Sarkar</strong>, Mahdi Rad, Vincent Lepetit
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR), 2022</em> <br>
                üèÜ <papertitle>Oral</papertitle> (top 4.2%)
                <br>
                <a href="https://arxiv.org/pdf/2104.14639.pdf"> <i class="fas fa-file-pdf"></i> arXiv</a> |       
                <a href="https://www.tugraz.at/index.php?id=57823"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://www.youtube.com/watch?v=D9YjoJnj_M4"> <i class="fas fa-video"></i> Video</a> |
                <a href="https://github.com/shreyashampali/kypt_transformer"> <i class="fab fa-github"></i> Code</a>
              
                <p></p>
                <p> Efficient network for joint two-hand and object pose estimation in complex interactions, paired with the new H2O-3D dataset of two-hand interaction with YCB objects. </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/mcss_cvpr21.png' width="250">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>Monte Carlo Scene Search for 3D Scene Understanding</papertitle>
                <br>
                Shreyas Hampali*, Sinisa Stekovic*, <strong>Sayan Deb Sarkar</strong>, Chetan Srinivasa Kumar, Friedrich Fraundorfer, Vincent Lepetit
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR), 2021</em>
                <br>
                <a href="https://arxiv.org/pdf/2103.07969.pdf"> <i class="fas fa-file-pdf"></i> arXiv </a> |
                <a href="https://www.tugraz.at/index.php?id=50484"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://www.youtube.com/watch?v=4kAfuymevUw&feature=youtu.be"> <i class="fas fa-video"></i> Video</a> |
                <a href="https://github.com/montescene"> <i class="fab fa-github"></i> Code</a>
                <p></p>
                <p> Monte-Carlo Tree Search (MCTS) based analysis-by-synthesis method to recover complete scene (3D layout+objects) from a noisy RGB-D scan. <br> 
              </td>
            </tr>

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="three">
                  <img src='images/roomlayout_eccv2020.png' width="250">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:top">
                  <papertitle>General 3D Room Layout from a Single View by Render-and-Compare</papertitle>
                <br>
                Sinisa Stekovic, Shreyas Hampali, Mahdi Rad, <strong>Sayan Deb Sarkar</strong>, Friedrich Fraundorfer, Vincent Lepetit
                <br>
                <em>European Conference on Computer Vision (ECCV), 2020</em>
                <br>
                <a href="https://arxiv.org/pdf/2001.02149.pdf"> <i class="fas fa-file-pdf"></i> arXiv </a> |
                <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/general-3d-room-layout-from-a-single-view-by-render-and-compare/"> <i class="fa fa-cube"></i> Project Page</a> |
                <a href="https://youtu.be/ZLNnGNzzE7k"> <i class="fas fa-video"></i> Video </a> |
                <a href="https://github.com/vevenom/RoomLayout3D_RandC"> <i class="fab fa-github"></i> Code</a>
                <p></p>
                <p> 3D layout estimation from a single perspective view, to recover complex non-cubiod layouts by solving a constrained discrete optimization problem. </p>
              </td>
            </tr>

        </tbody> </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Course Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td style="padding:20px;width:25%;vertical-align:top">
            <a href="images/final_scene_cg_as22.png"><img src="images/final_scene_cg_as22.png" alt="prl" width="250"></a></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p>
          <p>
            <a href="https://sayands.github.io/computer-graphics-project-report/">
            <papertitle>Ray Tracing</papertitle>
            </a>
            <br>
            <em> <a href="https://cgl.ethz.ch/teaching/cg22/home.php">Computer Graphics Rendering Competition</a>, Autumn Semester 2022 </em>
          </p>
          <p>
            Implemented a ray tracer with functionalities such as advanced camera models, participating media, photon mapping, Disney BRDF, etc on the 
            <strong>Nori</strong> framework. 
          </p>
          </p>
          </td>
        </tr>
        
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Misc</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Workshop Organisation:</b> <a href="https://cv4aec.github.io/">CV4AEC@CVPR</a> 2023, 2024
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Review:</b> CVPR, ICCV, ECCV, NeurIPS, ICRA
              </li>
            </p>
          </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src="images/media/stanford_logo.png" width="180">
                </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                  Teaching Assistant (Lead), <a href="https://gradientspaces.stanford.edu/computer-vision-built-environment"><strong>Computer Vision For The Built Environment</strong></a>, Winter 2025
                    <br>
                </td>
              </tr>
            </table>

          </td>
          </tr>
        </table>

      </td>
    </tr>
  </table>

  

  <center>
    <p>
        Template adapted from <a href="https://jonbarron.info/">this awesome website</a>
    </p>
  </center>


</body>

</html>

<script>
  function toggleBlock(blockId) {
    var block = document.getElementById(blockId);
    if (block.style.display === 'none') {
      block.style.display = 'block';
    } else {
      block.style.display = 'none';
    }
  }
</script>
